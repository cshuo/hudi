diff --git a/hudi-common/src/test/java/org/apache/hudi/common/table/read/TestHoodieFileGroupReaderBase.java b/hudi-common/src/test/java/org/apache/hudi/common/table/read/TestHoodieFileGroupReaderBase.java
index df9e7c4793a5a..1bb513d6c7d24 100644
--- a/hudi-common/src/test/java/org/apache/hudi/common/table/read/TestHoodieFileGroupReaderBase.java
+++ b/hudi-common/src/test/java/org/apache/hudi/common/table/read/TestHoodieFileGroupReaderBase.java
@@ -111,7 +111,7 @@ public abstract class TestHoodieFileGroupReaderBase<T> {
   private static final List<HoodieFileFormat> DEFAULT_SUPPORTED_FILE_FORMATS = Arrays.asList(HoodieFileFormat.PARQUET, HoodieFileFormat.ORC);
   protected static List<HoodieFileFormat> supportedFileFormats;
   private static final String KEY_FIELD_NAME = "_row_key";
-  private static final String PRECOMBINE_FIELD_NAME = "timestamp";
+  private static final String ORDERING_FIELD_NAME = "timestamp";
   private static final String PARTITION_FIELD_NAME = "partition_path";
   private static final String RIDER_FIELD_NAME = "rider";
   @TempDir
@@ -208,7 +208,7 @@ public void testReadFileGroupWithMultipleOrderingFields() throws Exception {
     writeConfigs.put("hoodie.datasource.write.table.type", HoodieTableType.MERGE_ON_READ.name());
     // Use two precombine values - combination of timestamp and rider
     String orderingValues = "timestamp,rider";
-    writeConfigs.put("hoodie.datasource.write.precombine.field", orderingValues);
+    writeConfigs.put(HoodieTableConfig.ORDERING_FIELDS.key(), orderingValues);
     writeConfigs.put("hoodie.payload.ordering.field", orderingValues);
 
     try (HoodieTestDataGenerator dataGen = new HoodieTestDataGenerator(0xDEEF)) {
@@ -628,8 +628,8 @@ protected Map<String, String> getCommonConfigs(RecordMergeMode recordMergeMode,
     Map<String, String> configMapping = new HashMap<>();
     configMapping.put(KeyGeneratorOptions.RECORDKEY_FIELD_NAME.key(), KEY_FIELD_NAME);
     configMapping.put(KeyGeneratorOptions.PARTITIONPATH_FIELD_NAME.key(), PARTITION_FIELD_NAME);
-    configMapping.put("hoodie.datasource.write.precombine.field", PRECOMBINE_FIELD_NAME);
-    configMapping.put("hoodie.payload.ordering.field", PRECOMBINE_FIELD_NAME);
+    configMapping.put(HoodieTableConfig.ORDERING_FIELDS.key(), ORDERING_FIELD_NAME);
+    configMapping.put("hoodie.payload.ordering.field", ORDERING_FIELD_NAME);
     configMapping.put(HoodieTableConfig.HOODIE_TABLE_NAME_KEY, "hoodie_test");
     configMapping.put("hoodie.insert.shuffle.parallelism", "4");
     configMapping.put("hoodie.upsert.shuffle.parallelism", "4");
@@ -713,7 +713,7 @@ private static List<HoodieRecord> getExpectedHoodieRecordsWithOrderingValue(List
     return expectedHoodieRecords.stream().map(rec -> {
       RawTripTestPayload oldPayload = (RawTripTestPayload) rec.getData();
       try {
-        List<String> orderingFields = metaClient.getTableConfig().getPreCombineFields();
+        List<String> orderingFields = metaClient.getTableConfig().getOrderingFields();
         HoodieAvroRecord avroRecord = ((HoodieAvroRecord) rec);
         Comparable orderingValue = OrderingValues.create(orderingFields, field -> (Comparable) avroRecord.getColumnValueAsJava(avroSchema, field, new TypedProperties()));
         RawTripTestPayload newPayload = new RawTripTestPayload(Option.ofNullable(oldPayload.getJsonData()), oldPayload.getRowKey(), oldPayload.getPartitionPath(), null, false, orderingValue);
@@ -739,20 +739,20 @@ private void validateOutputFromFileGroupReaderWithExistingRecords(StorageConfigu
     boolean sortOutput = !containsBaseFile;
     List<HoodieTestDataGenerator.RecordIdentifier> actualRecordList = convertEngineRecords(
         readRecordsFromFileGroup(storageConf, tablePath, metaClient, fileSlices, avroSchema, recordMergeMode, false, sortOutput),
-        avroSchema, readerContext, metaClient.getTableConfig().getPreCombineFields());
+        avroSchema, readerContext, metaClient.getTableConfig().getOrderingFields());
     // validate size is equivalent to ensure no duplicates are returned
     assertEquals(expectedRecords.size(), actualRecordList.size());
     assertEquals(new HashSet<>(expectedRecords), new HashSet<>(actualRecordList));
     // validate records can be read from file group as HoodieRecords
     actualRecordList = convertHoodieRecords(
         readHoodieRecordsFromFileGroup(storageConf, tablePath, metaClient, fileSlices, avroSchema, recordMergeMode),
-        avroSchema, readerContext, metaClient.getTableConfig().getPreCombineFields());
+        avroSchema, readerContext, metaClient.getTableConfig().getOrderingFields());
     assertEquals(expectedRecords.size(), actualRecordList.size());
     assertEquals(new HashSet<>(expectedRecords), new HashSet<>(actualRecordList));
     // validate unmerged records
     actualRecordList = convertEngineRecords(
         readRecordsFromFileGroup(storageConf, tablePath, metaClient, fileSlices, avroSchema, recordMergeMode, true, false),
-        avroSchema, readerContext, metaClient.getTableConfig().getPreCombineFields());
+        avroSchema, readerContext, metaClient.getTableConfig().getOrderingFields());
     assertEquals(expectedUnmergedRecords.size(), actualRecordList.size());
     assertEquals(new HashSet<>(expectedUnmergedRecords), new HashSet<>(actualRecordList));
   }
@@ -888,8 +888,8 @@ private List<HoodieRecord<T>> readHoodieRecordsFromFileGroup(StorageConfiguratio
 
   private TypedProperties buildProperties(HoodieTableMetaClient metaClient, RecordMergeMode recordMergeMode) {
     TypedProperties props = new TypedProperties();
-    props.setProperty("hoodie.datasource.write.precombine.field", metaClient.getTableConfig().getPreCombineFieldsStr().orElse(""));
-    props.setProperty("hoodie.payload.ordering.field", metaClient.getTableConfig().getPreCombineFieldsStr().orElse(""));
+    props.setProperty(HoodieTableConfig.ORDERING_FIELDS.key(), metaClient.getTableConfig().getOrderingFieldsStr().orElse(""));
+    props.setProperty("hoodie.payload.ordering.field", metaClient.getTableConfig().getOrderingFieldsStr().orElse(""));
     props.setProperty(RECORD_MERGE_MODE.key(), recordMergeMode.name());
     if (recordMergeMode.equals(RecordMergeMode.CUSTOM)) {
       props.setProperty(RECORD_MERGE_STRATEGY_ID.key(), PAYLOAD_BASED_MERGE_STRATEGY_UUID);
@@ -949,14 +949,14 @@ private List<HoodieTestDataGenerator.RecordIdentifier> convertEngineRecords(List
   }
 
   private List<HoodieTestDataGenerator.RecordIdentifier> convertHoodieRecords(List<HoodieRecord<T>> records, Schema schema, HoodieReaderContext<T> readerContext,
-                                                                              List<String> preCombineFields) {
+                                                                              List<String> orderingFields) {
     TypedProperties props = new TypedProperties();
-    props.setProperty("hoodie.datasource.write.precombine.field", String.join(",", preCombineFields));
+    props.setProperty(HoodieTableConfig.ORDERING_FIELDS.key(), String.join(",", orderingFields));
     return records.stream()
         .map(record -> new HoodieTestDataGenerator.RecordIdentifier(
             record.getRecordKey(),
             removeHiveStylePartition(record.getPartitionPath()),
-            record.getOrderingValue(schema, props, preCombineFields.toArray(new String[0])).toString(),
+            record.getOrderingValue(schema, props, orderingFields.toArray(new String[0])).toString(),
             readerContext.getRecordContext().getValue(record.getData(), schema, RIDER_FIELD_NAME).toString()))
         .collect(Collectors.toList());
   }
