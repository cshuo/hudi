diff --git a/hudi-examples/hudi-examples-spark/src/main/scala/org/apache/hudi/examples/spark/HoodieMorCompactionJob.scala b/hudi-examples/hudi-examples-spark/src/main/scala/org/apache/hudi/examples/spark/HoodieMorCompactionJob.scala
index 0fbb2a007a2c1..269480a58783f 100644
--- a/hudi-examples/hudi-examples-spark/src/main/scala/org/apache/hudi/examples/spark/HoodieMorCompactionJob.scala
+++ b/hudi-examples/hudi-examples-spark/src/main/scala/org/apache/hudi/examples/spark/HoodieMorCompactionJob.scala
@@ -24,6 +24,7 @@ import org.apache.hudi.QuickstartUtils.getQuickstartWriteConfigs
 import org.apache.hudi.client.SparkRDDWriteClient
 import org.apache.hudi.client.common.HoodieSparkEngineContext
 import org.apache.hudi.common.model.{HoodieAvroPayload, HoodieRecordPayload, HoodieTableType}
+import org.apache.hudi.common.table.HoodieTableConfig
 import org.apache.hudi.common.util.Option
 import org.apache.hudi.config.HoodieWriteConfig.TBL_NAME
 import org.apache.hudi.config.{HoodieCompactionConfig, HoodieWriteConfig}
@@ -89,7 +90,7 @@ object HoodieMorCompactionJob {
     val df = spark.read.json(spark.sparkContext.parallelize(inserts, 1))
     df.write.format("hudi").
       options(getQuickstartWriteConfigs).
-      option(PRECOMBINE_FIELD.key, "ts").
+      option(HoodieTableConfig.ORDERING_FIELDS.key, "ts").
       option(RECORDKEY_FIELD.key, "uuid").
       option(PARTITIONPATH_FIELD.key, "partitionpath").
       option(TBL_NAME.key, tableName).
@@ -105,7 +106,7 @@ object HoodieMorCompactionJob {
     val df = spark.read.json(spark.sparkContext.parallelize(updates, 1))
     df.write.format("hudi").
       options(getQuickstartWriteConfigs).
-      option(PRECOMBINE_FIELD.key, "ts").
+      option(HoodieTableConfig.ORDERING_FIELDS.key, "ts").
       option(RECORDKEY_FIELD.key, "uuid").
       option(PARTITIONPATH_FIELD.key, "partitionpath").
       option(TBL_NAME.key, tableName).
