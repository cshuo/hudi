diff --git a/hudi-examples/hudi-examples-spark/src/main/scala/org/apache/hudi/examples/spark/HoodieDataSourceExample.scala b/hudi-examples/hudi-examples-spark/src/main/scala/org/apache/hudi/examples/spark/HoodieDataSourceExample.scala
index a319bc1812288..0bfa4c2106222 100644
--- a/hudi-examples/hudi-examples-spark/src/main/scala/org/apache/hudi/examples/spark/HoodieDataSourceExample.scala
+++ b/hudi-examples/hudi-examples-spark/src/main/scala/org/apache/hudi/examples/spark/HoodieDataSourceExample.scala
@@ -22,9 +22,9 @@ import org.apache.hudi.DataSourceReadOptions.{END_COMMIT, QUERY_TYPE, QUERY_TYPE
 import org.apache.hudi.DataSourceWriteOptions.{DELETE_OPERATION_OPT_VAL, DELETE_PARTITION_OPERATION_OPT_VAL, OPERATION, PARTITIONPATH_FIELD, PARTITIONS_TO_DELETE, PRECOMBINE_FIELD, RECORDKEY_FIELD}
 import org.apache.hudi.QuickstartUtils.getQuickstartWriteConfigs
 import org.apache.hudi.common.model.HoodieAvroPayload
+import org.apache.hudi.common.table.HoodieTableConfig
 import org.apache.hudi.config.HoodieWriteConfig.TBL_NAME
 import org.apache.hudi.examples.common.{HoodieExampleDataGenerator, HoodieExampleSparkUtils}
-
 import org.apache.spark.sql.SaveMode.{Append, Overwrite}
 import org.apache.spark.sql.SparkSession
 
@@ -78,7 +78,7 @@ object HoodieDataSourceExample {
     val df = spark.read.json(spark.sparkContext.parallelize(inserts, 1))
     df.write.format("hudi").
       options(getQuickstartWriteConfigs).
-      option(PRECOMBINE_FIELD.key, "ts").
+      option(HoodieTableConfig.ORDERING_FIELDS.key, "ts").
       option(RECORDKEY_FIELD.key, "uuid").
       option(PARTITIONPATH_FIELD.key, "partitionpath").
       option(TBL_NAME.key, tableName).
@@ -120,7 +120,7 @@ object HoodieDataSourceExample {
     val df = spark.read.json(spark.sparkContext.parallelize(updates, 1))
     df.write.format("hudi").
       options(getQuickstartWriteConfigs).
-      option(PRECOMBINE_FIELD.key, "ts").
+      option(HoodieTableConfig.ORDERING_FIELDS.key, "ts").
       option(RECORDKEY_FIELD.key, "uuid").
       option(PARTITIONPATH_FIELD.key, "partitionpath").
       option(TBL_NAME.key, tableName).
@@ -139,7 +139,7 @@ object HoodieDataSourceExample {
 
     df.write.format("hudi").
       options(getQuickstartWriteConfigs).
-      option(PRECOMBINE_FIELD.key, "ts").
+      option(HoodieTableConfig.ORDERING_FIELDS.key, "ts").
       option(RECORDKEY_FIELD.key, "uuid").
       option(PARTITIONPATH_FIELD.key, "partitionpath").
       option(TBL_NAME.key, tableName).
@@ -155,7 +155,7 @@ object HoodieDataSourceExample {
     val df = spark.emptyDataFrame
     df.write.format("hudi").
       options(getQuickstartWriteConfigs).
-      option(PRECOMBINE_FIELD.key, "ts").
+      option(HoodieTableConfig.ORDERING_FIELDS.key, "ts").
       option(RECORDKEY_FIELD.key, "uuid").
       option(PARTITIONPATH_FIELD.key, "partitionpath").
       option(TBL_NAME.key, tableName).
